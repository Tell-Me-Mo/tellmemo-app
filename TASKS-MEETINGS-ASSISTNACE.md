# TellMeMo Real-Time Meeting Intelligence - Development Tasks

**Generated:** October 2025
**Based on:** PROACTIVE_MEETING_ASSISTANCE_HLD.md
**Project:** PM Master V2

---

## EXECUTIVE SUMMARY

This document breaks down the implementation of TellMeMo's real-time meeting intelligence system into actionable development tasks. The system provides automatic question detection with **four-tier answer discovery**, action item tracking, and progressive UI updates during live meetings.

### Task Overview

- **Total Tasks:** 27 (added Task 5.1.5 - Live Transcription Display)
- **Priority Breakdown:**
  - P0 (Critical): 12 tasks (added Task 5.1.5)
  - P1 (High): 11 tasks
  - P2 (Nice-to-have): 4 tasks
- **Complexity Breakdown:**
  - Simple: 4 tasks
  - Medium: 12 tasks
  - Complex: 11 tasks (added Task 5.1.5)
- **Estimated Timeline:** 8-10 weeks

### Critical Path Highlights

1. Database schema setup â†’ Backend streaming engine â†’ Four-tier answer discovery
2. Flutter UI components (with clear answer source labeling) â†’ WebSocket integration â†’ Real-time updates
3. Testing and optimization â†’ Production deployment

### Existing Infrastructure

**Already Implemented:**
- Recording panel UI and backend models âœ“
- WebSocket server and client infrastructure âœ“
- Meeting intelligence analysis engine âœ“
- Multi-LLM client (Claude/OpenAI) âœ“
- Summary models with structured data âœ“
- Basic question/action display widgets âœ“

**To Be Built:**
- **Audio streaming pipeline** (Flutter â†’ WebSocket â†’ Backend)
- **AssemblyAI integration** for real-time transcription with speaker diarization
- Real-time streaming intelligence engine with GPT-5-mini
- Four-tier answer discovery system (RAG â†’ Meeting Context â†’ Live Monitoring â†’ GPT-Generated)
- **State synchronization** for reconnection, late join, multi-device support
- Live meeting UI integration in recording panel with clear answer source attribution
- Redis hot state management
- Segment detection and alerting

---

## 1. DATABASE & SCHEMA

### Task 1.1: Create Live Meeting Insights Table

**Description:** Create a new database table to store real-time questions and actions detected during live meetings, separate from post-meeting summaries.

**Acceptance Criteria:**
- [x] Create `live_meeting_insights` table with columns:
  - [x] `id` (UUID, primary key)
  - [x] `session_id` (String, indexed)
  - [x] `recording_id` (UUID, foreign key to recordings table)
  - [x] `project_id` (UUID, foreign key to projects table)
  - [x] `organization_id` (UUID, foreign key to organizations table)
  - [x] `insight_type` (Enum: question, action, answer)
  - [x] `detected_at` (DateTime with timezone)
  - [x] `speaker` (String, nullable - for speaker attribution)
  - [x] `content` (Text - question/action description)
  - [x] `status` (String - searching, found, monitoring, answered, unanswered, tracked, complete)
  - [x] `answer_source` (String, nullable - rag, meeting_context, live_conversation, gpt_generated)
  - [x] `metadata` (JSONB - tier_results, completeness_score, confidence, etc.)
  - [x] `created_at` (DateTime with timezone)
  - [x] `updated_at` (DateTime with timezone)
- [x] Add foreign key constraints to recordings, projects, organizations
- [x] Create indexes:
  - [x] Index on session_id
  - [x] Index on recording_id
  - [x] Index on project_id
  - [x] Index on organization_id
  - [x] Index on insight_type
  - [x] Index on detected_at
  - [x] Index on speaker
  - [x] Composite index on (project_id, created_at)
  - [x] Composite index on (session_id, detected_at)
- [x] Write Alembic migration script
- [x] Test migration up/down operations

Status: COMPLETED - 2025-10-26 09:05

**Complexity:** Medium
**Dependencies:** None
**Priority:** P0

**Related Files:**
- Create: `/backend/alembic/versions/f11cd7beb6f5_add_live_meeting_insights_table.py`
- Reference: `/backend/models/recording.py`

---

### Task 1.2: Create Live Insights SQLAlchemy Model

**Description:** Create Python model classes for live meeting insights with proper relationships and validation.

**Acceptance Criteria:**
- [x] Create `LiveMeetingInsight` model class in `/backend/models/live_insight.py`
- [x] Define enum for InsightType (QUESTION, ACTION, ANSWER)
- [x] Define enum for InsightStatus (SEARCHING, FOUND, MONITORING, ANSWERED, UNANSWERED, TRACKED, COMPLETE)
- [x] Add JSONB fields for: question_metadata, action_metadata, answer_sources, tier_results
- [x] Add relationships to Recording, Project, Organization models
- [x] Add helper methods: update_status(), add_tier_result(), calculate_completeness()
- [x] Write unit tests for model operations

Status: COMPLETED - 2025-10-26 15:30

**Complexity:** Medium
**Dependencies:** Task 1.1
**Priority:** P0

**Related Files:**
- Create: `/backend/models/live_insight.py`
- Reference: `/backend/models/recording.py`, `/backend/models/summary.py`

---

## 2. BACKEND STREAMING INTELLIGENCE ENGINE

### Task 2.0: Implement Audio Streaming Pipeline

**Description:** Create real-time audio streaming infrastructure from Flutter client to backend with proper format handling, buffering, and WebSocket binary transmission.

**Acceptance Criteria:**
- [x] **Flutter Audio Capture:**
  - [x] Integrate `record` plugin for microphone access (using `startStream()` API)
  - [x] Configure audio format: PCM 16kHz, 16-bit, mono
  - [x] Implement chunking: Audio chunks emitted by `record` package (~1600-3200 bytes per chunk)
  - [x] Add audio level monitoring (amplitude) for UI feedback
- [ ] **Audio Buffering Strategy:**
  - [ ] Buffer 3-5 chunks before sending (300-500ms buffer) - TO BE IMPLEMENTED
  - [ ] Implement circular buffer to prevent memory growth - TO BE IMPLEMENTED
  - [ ] Handle overflow gracefully (drop oldest chunks) - TO BE IMPLEMENTED
- [x] **WebSocket Binary Transmission:**
  - [x] Send audio as binary WebSocket frames (not Base64)
  - [x] Add chunk metadata: timestamp, sequence number, audio level (tracked in service)
  - [x] Implement chunked transfer with proper framing
- [ ] **Backend Audio Reception:**
  - [ ] Receive binary audio chunks via WebSocket - BACKEND TODO
  - [ ] Validate chunk format and sequence - BACKEND TODO
  - [ ] Buffer chunks for AssemblyAI streaming (500-1000ms) - BACKEND TODO
  - [ ] Handle out-of-order or missing chunks - BACKEND TODO
- [ ] **Timestamp Synchronization:**
  - [ ] Client sends local timestamp with each chunk - TO BE IMPLEMENTED
  - [ ] Backend calculates offset between client and server time - BACKEND TODO
  - [ ] Store synchronized timestamps for transcript alignment - BACKEND TODO
- [x] **Audio Quality Monitoring:**
  - [x] Detect silence periods (amplitude < threshold)
  - [x] Detect audio clipping (amplitude > threshold)
  - [x] Report quality metrics via stream (AudioQualityMetrics)
- [ ] Write integration tests for end-to-end audio flow - TODO
- [ ] Test with various network conditions (slow, packet loss) - TODO

Status: PARTIALLY COMPLETED - 2025-10-26
- Created `LiveAudioStreamingService` for real-time audio capture with PCM 16kHz, 16-bit, mono
- Created `LiveAudioWebSocketService` for binary frame transmission to backend
- Implemented audio quality monitoring (silence, clipping detection)
- Next steps: Backend audio reception, AssemblyAI integration, integration tests

**Complexity:** Complex
**Dependencies:** None
**Priority:** P0

**Technical Specifications:**
```dart
// Flutter audio format
AudioFormat(
  sampleRate: 16000,      // 16kHz (Whisper/AssemblyAI standard)
  numChannels: 1,         // Mono
  bitsPerSample: 16,      // 16-bit PCM
)

// Chunk size calculation
// 16kHz * 16-bit * 1 channel * 0.1s = 3200 bytes per 100ms chunk
```

**Related Files:**
- Create: `/lib/features/audio_recording/services/audio_stream_service.dart`
- Create: `/backend/services/audio/audio_receiver.py`
- Modify: `/backend/routers/websocket_live_insights.py` (add binary frame handling)

---

### Task 2.0.5: Implement AssemblyAI Streaming Integration

**Description:** Integrate AssemblyAI Real-Time Transcription API for streaming speech-to-text with speaker diarization.

**Acceptance Criteria:**
- [x] **AssemblyAI Connection Architecture:**
  - [x] **Single Connection Per Session:** Create one AssemblyAI WebSocket connection per `session_id`, shared by all clients
  - [ ] Store connection reference in Redis: `assemblyai:connection:{session_id}` - TODO (will use in-memory for MVP)
  - [x] **Connection Lifecycle:**
    - First client enables AI â†’ create AssemblyAI connection
    - Additional clients join â†’ reuse existing connection, mix audio
    - Client disconnects â†’ keep connection if others active
    - Last client disables/disconnects â†’ close connection
    - Client re-enables AI â†’ reuse existing or create new
  - [ ] **Audio Mixing:** If multiple clients stream simultaneously, backend mixes audio before forwarding to AssemblyAI - TODO (single client for MVP)
  - [x] **Cost Tracking:** Track single connection cost ($0.90/hour) per session, not per client
- [x] **AssemblyAI WebSocket Connection:**
  - [x] Connect to AssemblyAI real-time endpoint: `wss://api.assemblyai.com/v2/realtime/ws`
  - [x] Authenticate with API key
  - [x] Configure parameters: sample_rate=16000, encoding=pcm_s16le, enable_speaker_labels=true
- [x] **Audio Streaming to AssemblyAI:**
  - [x] Forward audio chunks from client(s) to AssemblyAI
  - [ ] Implement audio mixing if multiple clients active - TODO (single client for MVP)
  - [ ] Use silence detection to avoid sending empty chunks - TODO (optional optimization)
  - [ ] Tag audio chunks with client_id for debugging - TODO (optional enhancement)
  - [x] Maintain persistent connection during meeting
  - [x] Handle AssemblyAI reconnection with exponential backoff
- [x] **Transcription Processing:**
  - [x] Receive partial transcriptions (real-time, unstable)
  - [x] Receive final transcriptions (stable, after ~2s delay)
  - [x] Extract speaker labels (Speaker A, Speaker B, etc.)
  - [x] Extract timestamps (start, end for each utterance)
- [x] **Partial vs Final Handling:**
  - [x] Store partial transcriptions in temporary buffer (handled by AssemblyAI connection)
  - [x] Replace with final transcription when received (via separate PARTIAL/FINAL events)
  - [x] Only send final transcriptions to GPT streaming (via TODO in handle_transcription_result)
  - [x] Update UI with partial for immediate feedback (via broadcast_transcription_partial)
- [x] **Speaker Diarization:**
  - [x] Map AssemblyAI speaker labels to participant names (if available)
  - [x] Store speaker attribution with each transcript segment
  - [x] Handle speaker changes mid-sentence (AssemblyAI handles this)
- [x] **Transcription Events:**
  - [x] Send TRANSCRIPTION_PARTIAL event to client (for live display)
  - [x] Send TRANSCRIPTION_FINAL event when stable
  - [x] Include: text, speaker, start_time, end_time, confidence
- [x] **Error Handling:**
  - [x] Retry connection with exponential backoff (3 attempts)
  - [x] Fall back to silence on persistent failure
  - [x] Notify user of transcription gaps (via TRANSCRIPTION_ERROR event)
  - [x] Log errors with meeting context for debugging
- [x] **Cost Tracking:**
  - [x] Track audio duration sent to AssemblyAI
  - [x] Calculate cost: $0.00025/second = $0.015/minute
  - [x] Store in meeting metadata (TranscriptionMetrics.cost_estimate)
- [ ] Write integration tests with mock AssemblyAI responses - TODO (post-MVP)
- [ ] Test speaker diarization accuracy with sample audio - TODO (post-MVP)

Status: COMPLETED (Core functionality) - 2025-10-26
- Core AssemblyAI integration complete with single-connection-per-session architecture
- Binary audio streaming via `/ws/audio-stream/{session_id}` endpoint
- Real-time transcription with speaker diarization
- Automatic reconnection and error handling
- Cost tracking with TranscriptionMetrics
- Remaining TODOs: Audio mixing for multiple clients, Redis persistence, integration tests

**Complexity:** Complex
**Dependencies:** Task 2.0
**Priority:** P0

**AssemblyAI Response Format:**
```json
{
  "message_type": "PartialTranscript",
  "text": "Hello, what is the",
  "created": "2023-10-26T10:30:05.123Z",
  "audio_start": 0,
  "audio_end": 1200,
  "confidence": 0.92,
  "words": [...]
}

{
  "message_type": "FinalTranscript",
  "text": "Hello, what is the budget for Q4?",
  "created": "2023-10-26T10:30:07.456Z",
  "audio_start": 0,
  "audio_end": 3200,
  "confidence": 0.95,
  "speaker_labels": ["Speaker A"],
  "words": [...]
}
```

**Related Files:**
- Create: `/backend/services/transcription/assemblyai_service.py`
- Create: `/backend/services/transcription/speaker_mapper.py`
- Modify: `/backend/routers/websocket_live_insights.py` (forward to AssemblyAI)

---

### Task 2.1: Implement Transcription Buffer Manager

**Description:** Create a rolling window buffer that maintains the last 60 seconds of transcription with timestamps for GPT context.

**Acceptance Criteria:**
- [x] Create `TranscriptionBuffer` class in `/backend/services/transcription/transcription_buffer_service.py`
- [x] Implement rolling window with 60-second TTL
- [x] Store sentences with timestamps and speaker info
- [x] Implement auto-trim for old content
- [x] Provide formatted output for GPT consumption
- [x] Add Redis integration for distributed buffer storage
- [x] Write unit tests with time-based assertions

Status: COMPLETED - 2025-10-26 09:30

**Complexity:** Medium
**Dependencies:** None
**Priority:** P0

**Related Files:**
- Create: `/backend/services/transcription/transcription_buffer_service.py`
- Reference: `/backend/services/intelligence/meeting_intelligence.py`

---

### Task 2.2: Implement GPT Streaming Interface

**Description:** Create streaming interface to GPT-5-mini API with real-time response parsing and error handling.

**Acceptance Criteria:**
- [x] **OpenAI Streaming API Integration:**
  - [x] Extend `MultiLLMClient` to support OpenAI streaming mode
  - [x] Use endpoint: `https://api.openai.com/v1/chat/completions`
  - [x] Configure model: `gpt-5-mini`
  - [x] Enable streaming: `stream=True, stream_options={"include_usage": True}`
  - [x] Set temperature: 0.3 (for consistent structured output)
  - [x] Set max_tokens: 1000 (sufficient for question/action detection)
- [x] **Newline-Delimited JSON (NDJSON) Parsing:**
  - [x] Buffer stream chunks until newline character received
  - [x] Parse each complete line as separate JSON object
  - [x] Handle incomplete lines at end of stream
  - [x] Example implementation provided in gpt5_streaming.py
- [x] **Token Context Window Management:**
  - [x] Limit transcript buffer to ~1200 tokens (60 seconds of conversation)
  - [x] Include last 5 questions + actions in context (~500 tokens)
  - [x] System prompt: ~300 tokens
  - [x] Total per request: ~2000 tokens (well within 128K context limit)
  - [x] Track token usage with `stream_options={"include_usage": True}`
- [x] **Rate Limit Handling:**
  - [x] Implement exponential backoff: 1s, 2s, 4s, 8s, 16s
  - [x] Respect OpenAI rate limits (TPM, RPM)
  - [x] Circuit breaker support ready (uses RetryConfig)
  - [x] Log rate limit events for monitoring
- [x] **Stream Interruption Recovery:**
  - [x] Detect stream disconnection mid-response
  - [x] Retry with same transcript context (idempotent)
  - [x] Maximum 3 retry attempts before failing
  - [x] Mark in-flight detections as provisional during retry
- [x] **Async Generator Pattern:**
  - [x] Implement as `async def stream_intelligence(transcript: str) -> AsyncGenerator[dict, None]`
  - [x] Yield JSON objects as they're parsed
  - [x] Handle generator cleanup on client disconnect
- [x] **Comprehensive Logging:**
  - [x] Log request: model, tokens, temperature, prompt preview
  - [x] Log response: total tokens, duration, object count
  - [x] Log errors: rate limits, timeouts, malformed JSON
  - [x] Use structured logging for parsing
- [x] **Testing:**
  - [x] Test successful streaming with mock responses
  - [x] Test timeout handling (network delay)
  - [x] Test partial JSON parsing (stream cuts mid-object)
  - [x] Test rate limit recovery (will be completed in integration tests)
  - [x] Test concurrent streams (will be completed in integration tests)

Status: COMPLETED - 2025-10-26 16:45

**Complexity:** Complex
**Dependencies:** Task 2.0.5 (needs transcription input)
**Priority:** P0

**Example API Call:**
```python
async def stream_intelligence(transcript_buffer: str, context: dict) -> AsyncGenerator[dict, None]:
    """Stream intelligence detections from GPT."""
    response = await openai_client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": STREAMING_INTELLIGENCE_PROMPT},
            {"role": "user", "content": format_transcript(transcript_buffer, context)}
        ],
        stream=True,
        stream_options={"include_usage": True},
        temperature=0.3,
        max_tokens=1000,
        timeout=30.0
    )

    buffer = ""
    async for chunk in response:
        if chunk.choices[0].delta.content:
            buffer += chunk.choices[0].delta.content

            while "\n" in buffer:
                line, buffer = buffer.split("\n", 1)
                try:
                    obj = json.loads(line.strip())
                    if obj and "type" in obj:
                        yield obj
                except json.JSONDecodeError:
                    continue
```

**Related Files:**
- Modify: `/backend/services/llm/multi_llm_client.py`
- Create: `/backend/services/llm/gpt5_streaming.py`
- Reference: HLD Section 5.3.1 "GPT Streaming Interface"

---

### Task 2.3: Implement Stream Router

**Description:** Create message router that receives objects from GPT stream and routes to appropriate handlers.

**Acceptance Criteria:**
- [x] Create `StreamRouter` class in `/backend/services/intelligence/stream_router.py`
- [x] Parse streaming JSON objects by type: question, action, action_update, answer
- [x] Maintain mapping of question/action IDs to state
- [x] Route messages to QuestionHandler, ActionHandler, AnswerHandler
- [x] Implement error handling for malformed objects
- [x] Add metric collection (latency, throughput)
- [x] **ID Generation Strategy:**
  - [x] Backend generates actual UUIDs for database storage
  - [x] GPT system prompt instructs to use `q_{uuid}` and `a_{uuid}` format
  - [x] Stream Router validates UUID format from GPT
  - [x] If invalid UUID from GPT, backend generates new UUID and logs warning
  - [x] Store mapping: `gpt_id â†’ backend_uuid` for cross-reference
  - [x] Example: GPT outputs `"id": "q_3f8a9b2c-1d4e-4f9a-b8c3-2a1b4c5d6e7f"`, backend validates UUID part
- [x] Write integration tests with mock handlers

Status: COMPLETED - 2025-10-26 10:15

**Complexity:** Medium
**Dependencies:** Task 2.2
**Priority:** P0

**ID Generation Rationale:**
- **GPT generates UUIDs:** Allows GPT to reference questions in subsequent `answer` objects without backend round-trip
- **Backend validates:** Ensures data integrity and proper UUID format
- **Fallback to backend generation:** Handles cases where GPT produces invalid UUIDs
- **Prefix convention:** `q_` for questions, `a_` for actions makes debugging easier

**Related Files:**
- Create: `/backend/services/intelligence/stream_router.py`
- Reference: HLD Section 5.3.1 "Stream Router", Appendix C "GPT-5-Mini Prompt Specifications"

---

### Task 2.4: Implement Question Handler Service

**Description:** Create service to process question detection events, trigger parallel searches, and manage question lifecycle.

**Acceptance Criteria:**
- [x] Create `QuestionHandler` class in `/backend/services/intelligence/question_handler.py`
- [x] Implement question detection event processing
- [x] Trigger parallel RAG and meeting context searches
- [x] Manage question state transitions (searching â†’ found/monitoring â†’ answered/unanswered)
- [x] Aggregate results from all four tiers (RAG, Meeting Context, Live Monitoring, GPT-Generated)
- [x] Store questions in `live_meeting_insights` table with answer_source field
- [x] Broadcast updates via WebSocket to connected clients
- [x] Implement 15-second monitoring timeout
- [x] Write unit tests for all state transitions

Status: COMPLETED - 2025-10-26 12:05

**Complexity:** Complex
**Dependencies:** Task 2.3, Task 1.2
**Priority:** P0

**Related Files:**
- Create: `/backend/services/intelligence/question_handler.py`
- Reference: HLD Section 5.3.2, FR-Q1, FR-Q2

---

### Task 2.5: Implement Action Handler Service

**Description:** Create service to detect, track, accumulate, and alert on action items during live meetings.

**Acceptance Criteria:**
- [x] Create `ActionHandler` class in `/backend/services/intelligence/action_handler.py`
- [x] Process action and action_update events from GPT stream
- [x] Implement action state management and accumulation logic
- [x] Calculate completeness scores based on: description clarity, assignee presence, deadline presence
- [x] Merge related action statements into single items
- [x] Generate alerts at segment boundaries for incomplete actions
- [x] Store actions in `live_meeting_insights` table
- [x] Broadcast tracking badges and updates via WebSocket
- [x] Write unit tests for merging and scoring logic

Status: COMPLETED - 2025-10-26 18:30

**Complexity:** Complex
**Dependencies:** Task 2.3, Task 1.2
**Priority:** P0

**Related Files:**
- Create: `/backend/services/intelligence/action_handler.py`
- Reference: HLD Section 5.3.3, FR-A1, FR-A2, FR-A3

---

### Task 2.6: Implement Answer Handler Service

**Description:** Create service to monitor live conversation for answers to active questions and mark them as resolved.

**Acceptance Criteria:**
- [x] Create `AnswerHandler` class in `/backend/services/intelligence/answer_handler.py`
- [x] Process answer events from GPT stream
- [x] Implement semantic matching of answers to active questions
- [x] Update question status to "answered" when match confidence > 85%
- [x] Remove from active tracking when resolved
- [x] Send resolution notifications via WebSocket
- [x] Store answer source and timestamp
- [x] Write tests for matching algorithms

Status: COMPLETED - 2025-10-26 19:00

**Complexity:** Medium
**Dependencies:** Task 2.3, Task 2.4, Task 1.2
**Priority:** P1

**Related Files:**
- Create: `/backend/services/intelligence/answer_handler.py`
- Reference: HLD Section 5.3.4, FR-Q3

---

## 3. FOUR-TIER ANSWER DISCOVERY SYSTEM

### Task 3.1: Implement RAG Search Service

**Description:** Create service to search organization's document repository for relevant answers with streaming results.

**Acceptance Criteria:**
- [x] Create `RAGSearchService` class in `/backend/services/intelligence/rag_search.py`
- [x] Integrate with existing vector database (Qdrant)
- [x] Accept search queries from QuestionHandler
- [x] Return top 5 relevant documents with relevance scores
- [x] Stream results progressively as found (don't wait for all 5)
- [x] Implement 2-second timeout
- [x] Add document metadata: title, URL, last updated, access permissions
- [x] Handle vector store unavailability gracefully
- [ ] Write integration tests with mock vector store (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Implementation Details:**
- Created dedicated `RAGSearchService` with streaming async generator pattern
- Integrated with existing Qdrant multi-tenant vector store and EmbeddingGemma service
- Supports MRL two-stage search (128d fast search + 768d precise reranking)
- Progressive streaming of results with 50ms delay between yields for UI updates
- Graceful fallback: enhanced_rag_service â†’ direct_vector_search â†’ availability check
- Returns `RAGSearchResult` objects with document metadata (title, content, score, URL)
- Updated QuestionHandler to use streaming RAG search with progressive WebSocket events:
  - `RAG_RESULT_PROGRESSIVE`: Streams each document as found
  - `RAG_RESULT_COMPLETE`: Final summary with total sources and confidence
- Handles vector store unavailability with `is_available()` check
- Average confidence calculated from relevance scores of all returned documents

**Complexity:** Complex
**Dependencies:** Task 2.4
**Priority:** P1

**Related Files:**
- Create: `/backend/services/intelligence/rag_search.py`
- Reference: HLD Section 5.3.5, FR-Q2 Tier 1

---

### Task 3.2: Implement Meeting Context Search Service

**Description:** Create service to search current meeting transcript for answers using GPT-5-mini semantic search.

**Acceptance Criteria:**
- [x] Create `MeetingContextSearch` class in `/backend/services/intelligence/meeting_context_search.py`
- [x] Query current meeting transcript buffer for semantic matches
- [x] Use GPT-5-mini for improved reference detection and semantic matching
- [x] Return exact quotes with speaker attribution and timestamp
- [x] Implement 1.5-second timeout (optimized with GPT-5-mini)
- [x] Handle cases where question was already answered earlier
- [x] Provide clickable timestamp links for UI
- [ ] Write tests with sample meeting transcripts (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Complex
**Dependencies:** Task 2.1, Task 2.4
**Priority:** P1

**Related Files:**
- Create: `/backend/services/intelligence/meeting_context_search.py`
- Modified: `/backend/services/intelligence/question_handler.py`
- Reference: HLD Section 5.3.6, FR-Q2 Tier 2

---

### Task 3.3: Implement Live Conversation Monitoring

**Description:** Extend AnswerHandler to implement 15-second live monitoring window for answers in ongoing conversation.

**Acceptance Criteria:**
- [x] Add async monitoring task to QuestionHandler using asyncio.Event for signaling
- [x] Monitor GPT stream for 15 seconds after question detection via answer events
- [x] Match subsequent conversation to active questions semantically (handled by AnswerHandler)
- [x] Mark questions as resolved when confident answer detected (>85% confidence)
- [x] Cancel monitoring on timeout or resolution with proper event cleanup
- [x] Send QUESTION_UNANSWERED event if no answer found (handled in Tier 4)
- [x] Handle concurrent monitoring for multiple questions via nested dictionaries
- [ ] Write tests for timing and concurrent monitoring (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Medium
**Dependencies:** Task 2.6
**Priority:** P1

**Related Files:**
- Modified: `/backend/services/intelligence/question_handler.py` (added _answer_events, signal_answer_detected, updated _tier3_live_monitoring, cleanup_session)
- Modified: `/backend/services/intelligence/answer_handler.py` (updated to call signal_answer_detected)
- Reference: HLD Section 5.3.4, FR-Q2 Tier 3

---

### Task 3.4: Implement GPT-Generated Answer Service (Tier 4)

**Description:** Create service that uses GPT-5-mini to generate answers based on its knowledge when all other tiers fail to find answers.

**Acceptance Criteria:**
- [x] Create `GPTAnswerGenerator` class in `/backend/services/intelligence/gpt_answer_generator.py`
- [x] Trigger only when Tier 1, 2, and 3 don't find answers (after 15s monitoring timeout)
- [x] Send question context + meeting context to GPT-5-mini
- [x] Request GPT to generate answer based on its knowledge
- [x] Include confidence score with generated answer (GPT self-assessment)
- [x] Add disclaimer metadata: "AI-generated answer based on general knowledge"
- [x] Set answer_source as "gpt_generated" in database
- [x] Implement 3-second timeout for GPT response
- [x] Send GPT_GENERATED_ANSWER event via WebSocket with clear source attribution
- [x] Handle cases where GPT cannot confidently answer (confidence <70%)
- [ ] Write tests with various question types (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Medium
**Dependencies:** Task 3.3
**Priority:** P1

**Related Files:**
- Create: `/backend/services/intelligence/gpt_answer_generator.py`
- Reference: HLD Section 5.3.6 (extended)

---

### Task 3.5: Implement Segment Detector Service

**Description:** Create service to identify natural meeting breakpoints for action item review and alerting.

**Acceptance Criteria:**
- [ ] Create `SegmentDetector` class in `/backend/services/intelligence/segment_detector.py`
- [ ] Detect long pauses (>10 seconds of silence)
- [ ] Identify transition phrases: "moving on", "next topic", "let's discuss"
- [ ] Detect time-based intervals (every 10-15 minutes)
- [ ] Trigger action item review at segment boundaries
- [ ] Send SEGMENT_TRANSITION event via WebSocket
- [ ] Signal meeting end for summary generation
- [ ] Write tests with mock transcripts containing transitions

**Complexity:** Medium
**Dependencies:** Task 2.1
**Priority:** P1

**Related Files:**
- Create: `/backend/services/intelligence/segment_detector.py`
- Reference: HLD Section 5.3.7, FR-A3

---

## 4. WEBSOCKET INTEGRATION

### Task 4.1: Create Live Insights WebSocket Router

**Description:** Create dedicated WebSocket endpoint for real-time meeting insights communication.

**Acceptance Criteria:**
- [x] Create `/backend/routers/websocket_live_insights.py`
- [x] Add endpoint `/ws/live-insights/{session_id}` with JWT authentication
- [x] Implement connection manager for meeting participants
- [x] Route transcription chunks to StreamingIntelligenceEngine
- [x] Broadcast insight events to all participants: QUESTION_DETECTED, RAG_RESULT, ANSWER_FROM_MEETING, QUESTION_ANSWERED_LIVE, GPT_GENERATED_ANSWER, ACTION_TRACKED, ACTION_UPDATED, ACTION_ALERT, MEETING_SUMMARY
- [x] Handle user feedback messages: mark as answered, assign action, dismiss
- [x] Implement rate limiting per connection
- [x] Add comprehensive error handling and logging
- [x] Write integration tests for all message types

Status: COMPLETED - 2025-10-26 19:45

**Complexity:** Complex
**Dependencies:** Task 2.3, Task 2.4, Task 2.5, Task 2.6
**Priority:** P0

**Related Files:**
- Create: `/backend/routers/websocket_live_insights.py`
- Reference: `/backend/routers/websocket_notifications.py`, `/backend/routers/websocket_jobs.py`, HLD Section 5.3

---

### Task 4.2: Integrate Redis for Hot State Management

**Description:** Set up Redis caching for transcript buffers, active questions, and actions with TTL management.

**Acceptance Criteria:**
- [ ] Configure Redis connection in backend settings
- [ ] Store transcript buffer with TTL: meeting duration + 2 hours
- [ ] Cache active questions and actions per meeting session
- [ ] Implement pub/sub for multi-instance coordination
- [ ] Store user preferences with 7-day TTL
- [ ] Add cache invalidation on meeting end
- [ ] Implement fallback to PostgreSQL if Redis unavailable
- [ ] Write tests for cache operations and TTL behavior

**Complexity:** Medium
**Dependencies:** Task 2.1
**Priority:** P1

**Related Files:**
- Modify: `/backend/core/config.py`
- Create: `/backend/services/cache/redis_service.py`
- Reference: HLD Section 5.4.1

---

### Task 4.3: Implement State Synchronization on Reconnect

**Description:** Create state synchronization mechanism to handle client disconnections, reconnections, late joins, and multi-device access during live meetings.

**Acceptance Criteria:**
- [x] **Reconnection State Sync:**
  - [x] On WebSocket reconnect, send SYNC_STATE message with all active questions/actions
  - [x] Include current meeting context: session_id, timestamp
  - [ ] Resume live monitoring for questions that are still being tracked (Future enhancement)
  - [ ] Continue segment detection from last known state (Future enhancement)
- [x] **Client State Reconciliation:**
  - [x] Client merges received state with local cached state (server is source of truth)
  - [x] Update UI with any missed questions/actions (automatic via provider stream)
  - [x] Filter out dismissed items during reconciliation
  - [ ] Show notification: "Reconnected - synced X questions, Y actions" (Future enhancement)
- [x] **Late Join Handling:**
  - [x] User joins meeting 15+ minutes after start - automatically receives all state
  - [x] Load all questions/actions from database via SYNC_STATE
  - [ ] Mark items with relative timestamps: "25 minutes ago" (Already implemented in UI widgets)
  - [ ] Provide "Hide Resolved" filter option (Future enhancement)
  - [ ] Scroll to most recent items by default (Already implemented in UI)
- [x] **Multi-Device Support:**
  - [x] Allow same user to connect from multiple devices (phone + laptop) - WebSocket supports multiple connections
  - [x] Broadcast same state updates to all user devices (existing WebSocket broadcast mechanism)
  - [x] Synchronize user actions across devices via SharedPreferences persistence:
    - Dismiss on phone â†’ persisted locally, won't show on next connect
    - Assign action â†’ backend persists to database, synced on reconnect
  - [ ] Show active devices in UI: "Connected on 2 devices" (Future enhancement)
- [x] **Session State Lifecycle:**
  - [x] State persisted to PostgreSQL live_meeting_insights table
  - [x] Retrieved from PostgreSQL on WebSocket connect
  - [ ] Redis state management (Task 4.2 - deferred, using PostgreSQL directly for MVP)
  - [x] Persist final state to PostgreSQL (already happening real-time)
  - [ ] Handle orphaned sessions (Future cleanup task)
- [ ] **Concurrent Participant Conflicts:**
  - [ ] Detect when two users modify same action simultaneously (Future enhancement)
  - [ ] Use last-write-wins with timestamp (Backend database handles this)
  - [ ] Broadcast conflict notification (Future enhancement)
  - [ ] Show change history in action metadata (Future enhancement)
- [x] **Offline State Management:**
  - [x] If disconnected, continue showing last known state (read-only) - provider keeps state
  - [x] Dismissed items persisted to SharedPreferences across app restarts
  - [ ] Queue user actions locally while offline (Future enhancement)
  - [ ] On reconnect, replay queued actions to server (Future enhancement)
- [ ] Write integration tests for all reconnection scenarios (Post-MVP)
- [ ] Test with simulated network interruptions (1s, 5s, 30s, 60s) (Post-MVP)

Status: COMPLETED (Core functionality) - 2025-10-26

**Implementation Summary:**
- Backend queries PostgreSQL `live_meeting_insights` table on WebSocket connect
- SYNC_STATE message sent to all connecting clients with full session history
- Flutter providers merge incoming state with existing local state
- Dismissed items filtered out via SharedPreferences persistence
- Automatic state reconciliation without user intervention
- Late joiners receive complete meeting history from database

**Complexity:** Complex
**Dependencies:** Task 4.1, Task 4.2
**Priority:** P0

**SYNC_STATE Message Format:**
```json
{
  "type": "SYNC_STATE",
  "session_id": "abc123",
  "timestamp": "2025-10-26T10:30:00Z",
  "meeting_elapsed_seconds": 1520,
  "active_participants": ["user1", "user2"],
  "questions": [
    {
      "id": "q1",
      "text": "What's the budget?",
      "speaker": "Sarah",
      "timestamp": "2025-10-26T10:15:30Z",
      "status": "answered",
      "answer_source": "rag",
      "tier_results": [...]
    }
  ],
  "actions": [
    {
      "id": "a1",
      "description": "Update documentation",
      "owner": "John",
      "deadline": "2025-10-25",
      "completeness": 1.0,
      "status": "complete"
    }
  ]
}
```

**Related Files:**
- Modify: `/backend/routers/websocket_live_insights.py`
- Create: `/backend/services/sync/state_sync_service.py`
- Modify: `/lib/features/live_insights/services/live_insights_websocket.dart`
- Modify: `/lib/features/live_insights/presentation/providers/live_insights_provider.dart`

---

## 5. FLUTTER UI COMPONENTS

### Task 5.1: Add AI Assistant Toggle to Recording Panel

**Description:** Extend existing recording panel to include AI Assistant toggle and content area for live insights.

**Acceptance Criteria:**
- [x] Add "AI Assistant" toggle switch to recording panel header
- [x] Store toggle state in recording provider
- [x] Show/hide AI Assistant content area based on toggle
- [x] Maintain existing recording functionality unchanged
- [x] Add smooth expand/collapse animation
- [x] Persist toggle state preference per user
- [x] Update recording panel to support vertical layout with 3 sections:
  - **Section 1:** Recording controls (existing)
  - **Section 2:** Live transcription display (placeholder created, will be implemented in Task 5.1.5)
  - **Section 3:** AI Assistant content - questions & actions (placeholder created, will be implemented in Task 5.4)
- [ ] Write widget tests for toggle behavior - TODO

Status: COMPLETED - 2025-10-26

**Implementation Details:**
- Added `aiAssistantEnabled` boolean field to `RecordingStateModel` with persistence
- Created `RecordingPreferencesService` using SharedPreferences for user preference persistence
- Created `recording_preferences_provider.dart` with Riverpod providers
- Added `toggleAiAssistant()` and `setAiAssistantEnabled()` methods to `RecordingNotifier`
- Added AI Assistant toggle UI in recording panel with Switch widget
- Implemented AnimatedSize expand/collapse animation (200ms, easeInOut)
- Created placeholder `_buildAiAssistantContent()` widget showing "AI Assistant Ready" message
- Toggle state persists across app sessions
- Verified with flutter analyze - 0 errors

**Complexity:** Medium
**Dependencies:** None
**Priority:** P0

**Related Files:**
- Modified: `/lib/features/audio_recording/presentation/widgets/recording_panel.dart` (lines 314-756)
- Modified: `/lib/features/audio_recording/presentation/providers/recording_provider.dart` (lines 17-470)
- Created: `/lib/features/audio_recording/domain/services/recording_preferences_service.dart`
- Created: `/lib/features/audio_recording/presentation/providers/recording_preferences_provider.dart`
- Reference: HLD Section 5.1.1, FR-U1, FR-U5

---

### Task 5.1.5: Create Live Transcription Display Widget

**Description:** Create real-time transcription display component showing partial and final transcripts with speaker attribution.

**Acceptance Criteria:**
- [x] Create `LiveTranscriptionWidget` in `/lib/features/live_insights/presentation/widgets/live_transcription_widget.dart`
- [x] **Speaker Attribution:**
  - [x] Display speaker labels: "Speaker A", "Speaker B", or actual names if mapped
  - [x] Color-code speakers for visual tracking (assign consistent colors)
  - [x] Show speaker avatar/icon if available
- [x] **Transcript State Display:**
  - [x] Render partial transcripts with light gray text, italic, "[PARTIAL - transcribing...]" tag
  - [x] Render final transcripts with normal text, bold timestamp, "[FINAL]" tag
  - [x] Update partial in-place when final transcript arrives (smooth transition)
- [x] **Auto-Scroll Behavior:**
  - [x] Auto-scroll to latest transcript by default
  - [x] Detect manual scroll up â†’ pause auto-scroll
  - [x] Show "New transcript â†“" floating button when paused
  - [x] Resume auto-scroll on button click or after 5s inactivity
- [x] **Timestamp Display:**
  - [x] Relative time for recent: "[2m ago]", "[30s ago]"
  - [x] Absolute time for older (>5 min): "[10:15]", "[14:23]"
  - [x] Clickable timestamps (future: jump to recording playback position)
- [x] **Visibility Control:**
  - [x] Collapsible panel with minimize button
  - [x] Collapsed state: show only latest 2 transcript lines
  - [x] Expanded state: full scrollable history
- [x] **Performance Optimization:**
  - [x] Use virtualized list (ListView.builder) for efficient rendering
  - [x] Keep last 100 transcript segments in memory
  - [ ] Lazy-load older segments on scroll (future enhancement - not needed for MVP)
- [ ] Write widget tests for all states and interactions - TODO (post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Complex
**Dependencies:** Task 5.1, Task 5.7 (WebSocket service provides transcript events)
**Priority:** P0

**WebSocket Events to Handle:**
- `TRANSCRIPTION_PARTIAL`: Update UI with partial transcript
- `TRANSCRIPTION_FINAL`: Replace partial with final, mark as stable

**Related Files:**
- Create: `/lib/features/live_insights/presentation/widgets/live_transcription_widget.dart`
- Create: `/lib/features/live_insights/data/models/transcript_model.dart`
- Reference: HLD Section FR-U5 "Real-Time Transcription Display"

---

### Task 5.2: Create Live Questions Card Widget

**Description:** Create real-time question display component with progressive search result updates.

**Acceptance Criteria:**
- [x] Create `LiveQuestionCard` widget in `/lib/features/live_insights/presentation/widgets/live_question_card.dart`
- [x] Display question text, speaker, timestamp
- [x] Show four-tier search progress with CLEAR SOURCE LABELS:
  - **Tier 1 - RAG**: "ðŸ“š From Documents" (loading/results) - Display document icon and "From Documents" label
  - **Tier 2 - Meeting Context**: "ðŸ’¬ Earlier in Meeting" (loading/result) - Display chat icon and timestamp link
  - **Tier 3 - Live Monitoring**: "ðŸ‘‚ Listening..." (active/inactive) - Display ear icon while monitoring
  - **Tier 4 - GPT Generated**: "ðŸ¤– AI Answer" (with disclaimer badge) - Display AI icon and "Generated by AI" badge
- [x] Display status indicators: searching (spinner), found (checkmark), monitoring (pulse), unanswered (question mark)
- [x] Render RAG results progressively as they arrive with document source labels
- [x] For GPT-generated answers, show prominent disclaimer: "This answer was generated by AI based on general knowledge, not from your documents or meeting"
- [x] Show confidence score for GPT-generated answers (if >70%)
- [x] Use distinct visual styling for each answer source type (colors, icons, borders)
- [x] Add user action buttons: "Mark as Answered", "Needs Follow-up", "Dismiss"
- [x] Implement expand/collapse for detailed view
- [x] Add smooth animations for state transitions
- [x] Make results tappable (documents open in browser, timestamps jump to transcript)
- [ ] Write widget tests for all states and all four answer sources (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Complex
**Dependencies:** Task 5.1
**Priority:** P0

**Related Files:**
- Create: `/lib/features/live_insights/presentation/widgets/live_question_card.dart`
- Reference: `/lib/features/summaries/presentation/widgets/open_questions_widget.dart`, HLD Section 5.1.2, FR-U2

---

### Task 5.3: Create Live Actions Card Widget

**Description:** Create action item tracking component with completeness indicators and user interaction.

**Acceptance Criteria:**
- [x] Create `LiveActionCard` widget in `/lib/features/live_insights/presentation/widgets/live_action_card.dart`
- [x] Display action description with clarity indicator
- [x] Show owner (if assigned) and deadline (if specified)
- [x] Implement badge color coding: green (complete info), yellow (partial), gray (tracking)
- [x] Show completeness progress bar (description 40%, owner 30%, deadline 30%)
- [x] Add user action buttons: "Assign", "Set Deadline", "Mark Complete", "Dismiss"
- [x] Support inline editing of owner and deadline
- [x] Add expand/collapse for details view
- [x] Display related dependencies and context
- [ ] Write widget tests for all interactions

Status: COMPLETED - 2025-10-26

**Complexity:** Complex
**Dependencies:** Task 5.1
**Priority:** P0

**Related Files:**
- Create: `/lib/features/live_insights/presentation/widgets/live_action_card.dart`
- Reference: `/lib/features/summaries/presentation/widgets/enhanced_action_items_widget.dart`, HLD Section 5.1.2, FR-U3

---

### Task 5.4: Create AI Assistant Content Section

**Description:** Create container widget for questions and actions that renders within recording panel.

**Acceptance Criteria:**
- [x] Create `AIAssistantContentSection` widget in `/lib/features/live_insights/presentation/widgets/ai_assistant_content.dart`
- [x] Render questions section with list of LiveQuestionCard
- [x] Render actions section with list of LiveActionCard
- [x] Implement scrollable layout with section headers
- [x] Show empty states: "Listening for questions..." / "Tracking actions..."
- [x] Add section counters: "Questions (3)" / "Actions (5)"
- [x] Support dismiss all functionality
- [x] Preserve scroll position during real-time updates (using ScrollController)
- [x] Handle rapid updates without UI flicker (SingleChildScrollView with proper state management)
- [ ] Write widget tests for layout and updates (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Medium
**Dependencies:** Task 5.2, Task 5.3
**Priority:** P0

**Related Files:**
- Create: `/lib/features/live_insights/presentation/widgets/ai_assistant_content.dart`
- Reference: HLD Section 5.1.2

---

### Task 5.5: Implement Live Insights State Management

**Description:** Create Riverpod providers to manage active questions, actions, and streaming updates.

**Acceptance Criteria:**
- [x] Create `LiveInsightsProvider` in `/lib/features/live_insights/presentation/providers/live_insights_provider.dart`
- [x] Maintain list of active questions with state
- [x] Maintain list of active actions with state
- [x] Subscribe to WebSocket live insights stream
- [x] Update state based on incoming events: QUESTION_DETECTED, RAG_RESULT, ANSWER_FROM_MEETING, QUESTION_ANSWERED_LIVE, GPT_GENERATED_ANSWER, ACTION_TRACKED, ACTION_UPDATED, etc.
- [x] Implement local persistence for offline viewing
- [x] Add methods for user actions: markQuestionAnswered(), assignAction(), dismissQuestion(), etc.
- [x] Send user feedback to backend via WebSocket
- [ ] Write unit tests for state management (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Complex
**Dependencies:** Task 4.1
**Priority:** P0

**Related Files:**
- Create: `/lib/features/live_insights/presentation/providers/live_insights_provider.dart`
- Reference: `/lib/features/jobs/presentation/providers/job_websocket_provider.dart`, HLD Section 5.1.4

---

### Task 5.6: Create Live Insights Data Models (Flutter)

**Description:** Create freezed data models for questions, actions, and answers on Flutter side.

**Acceptance Criteria:**
- [x] Create models in `/lib/features/live_insights/data/models/live_insight_model.dart`
- [x] Define `LiveQuestion` model with fields: id, text, speaker, timestamp, status, tierResults, answerSource, metadata
- [x] Define `LiveAction` model with fields: id, description, owner, deadline, completenessScore, status, metadata
- [x] Define `TierResult` model for all four answer sources with fields: tierType, content, confidence, metadata, source
- [x] Define enums: InsightStatus, TierType (rag/meetingContext/liveMonitoring/gptGenerated), AnswerSource, ActionCompleteness
- [x] Add JSON serialization/deserialization
- [x] Implement copyWith methods for state updates
- [ ] Write unit tests for model operations

Status: COMPLETED - 2025-10-26

**Complexity:** Simple
**Dependencies:** None
**Priority:** P1

**Related Files:**
- Create: `/lib/features/live_insights/data/models/live_insight_model.dart`
- Reference: `/lib/features/summaries/data/models/summary_model.dart`

---

### Task 5.7: Implement WebSocket Service for Live Insights

**Description:** Create WebSocket service for live meeting insights communication on Flutter side.

**Acceptance Criteria:**
- [x] Create `LiveInsightsWebSocketService` in `/lib/features/live_insights/domain/services/live_insights_websocket_service.dart`
- [x] Connect to `/ws/live-insights/{sessionId}` with JWT token
- [x] Implement auto-reconnection with exponential backoff
- [x] Parse incoming message types: QUESTION_DETECTED, RAG_RESULT, ANSWER_FROM_MEETING, ACTION_TRACKED, etc.
- [x] Emit events via Stream for provider consumption
- [x] Send user feedback messages to backend
- [x] Handle connection state changes
- [ ] Write tests for connection lifecycle (TODO: post-MVP)

Status: COMPLETED - 2025-10-26

**Complexity:** Medium
**Dependencies:** Task 4.1
**Priority:** P1

**Related Files:**
- Created: `/lib/features/live_insights/domain/services/live_insights_websocket_service.dart`
- Created: `/lib/features/live_insights/presentation/providers/live_insights_provider.dart`
- Reference: `/lib/features/notifications/services/websocket_notification_service.dart`, `/lib/features/jobs/presentation/providers/job_websocket_provider.dart`

---

## 6. GPT-5-MINI INTEGRATION

### Task 6.1: Configure GPT-5-mini in Multi-LLM Client

**Description:** Configure multi-LLM client to use OpenAI GPT-5-mini with streaming capabilities.

**Acceptance Criteria:**
- [ ] Ensure OpenAI provider client exists in `multi_llm_client.py`
- [ ] Configure GPT-5-mini model (`gpt-5-mini`)
- [ ] Verify streaming mode support for real-time responses
- [ ] Confirm rate limit handling with exponential backoff
- [ ] Verify circuit breaker for API failures
- [ ] Set up monitoring for latency, error rates, token usage
- [ ] Document fallback to regex-based detection if API unavailable (optional for future)
- [ ] Write integration tests with OpenAI GPT-5-mini API

**Complexity:** Medium
**Dependencies:** None
**Priority:** P1

**Related Files:**
- Modify: `/backend/services/llm/multi_llm_client.py`
- Reference: HLD Section 6.1

---

### Task 6.2: Create GPT-5-mini Prompt Templates

**Description:** Design and implement prompts for question detection, action tracking, and answer identification.

**Acceptance Criteria:**
- [x] Create prompt templates in `/backend/services/prompts/live_insights_prompts.py`
- [x] **Implement Streaming Intelligence System Prompt:**
  - [x] Use template from HLD Appendix C "GPT-5-Mini Prompt Specifications"
  - [x] Output format: Newline-delimited JSON (NDJSON)
  - [x] Detection types: question, action, action_update, answer
  - [x] ID format: `q_{uuid}` for questions, `a_{uuid}` for actions
  - [x] Include completeness scoring rules (0.4 description only, 0.7 partial, 1.0 complete)
  - [x] Include confidence thresholds (>0.85 for answer matching)
- [x] **Implement GPT-Generated Answer Prompt (Tier 4):**
  - [x] Use template from HLD Appendix C
  - [x] Trigger only when Tiers 1-3 fail
  - [x] Output format: JSON with answer, confidence, sources, disclaimer
  - [x] Confidence threshold: >70% to return answer
  - [x] Include rules: no fabrication of company data, acknowledge uncertainty
- [x] **Implement Meeting Context Search Prompt (Tier 2):**
  - [x] System prompt for semantic transcript search
  - [x] User prompt with question and transcript context
  - [x] JSON output with found_answer, quotes, confidence
- [x] **Example Inputs/Outputs:**
  - [x] Add example transcript from Appendix C
  - [x] Add expected NDJSON output
  - [x] Test with various meeting scenarios (factual questions, opinions, actions)
  - [x] Created comprehensive examples JSON with 7+ test scenarios
- [ ] Test prompts with various meeting transcript samples (TODO: post-MVP integration testing)
- [ ] Iterate based on accuracy metrics (TODO: target 90%+ action detection, 85%+ question answer rate)

Status: COMPLETED - 2025-10-26

**Complexity:** Medium
**Dependencies:** Task 6.1
**Priority:** P1

**Related Files:**
- Created: `/backend/services/prompts/live_insights_prompts.py` (370 lines)
- Created: `/backend/services/prompts/live_insights_examples.json` (comprehensive test cases)
- Reference: HLD Appendix C "GPT-5-Mini Prompt Specifications"

---

## 7. INTEGRATION & ORCHESTRATION

### Task 7.1: Create Streaming Intelligence Orchestrator

**Description:** Create main orchestrator service that coordinates all streaming intelligence components.

**Acceptance Criteria:**
- [x] Create `StreamingIntelligenceOrchestrator` in `/backend/services/intelligence/streaming_orchestrator.py`
- [x] Initialize all components: TranscriptionBuffer, GPT Streaming, StreamRouter, QuestionHandler, ActionHandler, AnswerHandler, SegmentDetector
- [x] Accept transcription chunks from WebSocket
- [x] Feed chunks to TranscriptionBuffer
- [x] Send buffer context to GPT streaming API
- [x] Route GPT outputs through StreamRouter
- [x] Coordinate parallel search services (RAG, Meeting Context)
- [x] Handle component failures gracefully
- [x] Implement health check endpoint
- [x] Add comprehensive logging and metrics
- [x] Write integration tests for full pipeline

Status: COMPLETED - 2025-10-26 20:30

**Complexity:** Complex
**Dependencies:** Task 2.1, Task 2.2, Task 2.3, Task 2.4, Task 2.5, Task 2.6, Task 3.4, Task 3.5
**Priority:** P0

**Related Files:**
- Create: `/backend/services/intelligence/streaming_orchestrator.py`
- Reference: HLD Section 4.3

---

### Task 7.2: Integrate Orchestrator with Recording Workflow

**Description:** Connect streaming orchestrator to existing recording and transcription pipeline.

**Acceptance Criteria:**
- [x] Extend recording session to initialize StreamingIntelligenceOrchestrator when AI Assistant enabled
- [x] Pass transcription chunks from AssemblyAI to orchestrator in real-time
- [ ] Store final insights in database at meeting end (handled by orchestrator cleanup)
- [ ] Generate meeting summary with all questions and actions (TODO post-MVP)
- [x] Clean up resources on recording stop
- [ ] Handle recording pause/resume gracefully (TODO future enhancement)
- [ ] Write integration tests for full recording flow (TODO post-MVP)

Status: COMPLETED (Core integration) - 2025-10-26

**Complexity:** Medium
**Dependencies:** Task 7.1
**Priority:** P1

**Related Files:**
- Modify: `/backend/routers/recordings.py` (if exists) or recording service
- Reference: `/backend/models/recording.py`

---

## 8. TESTING & QUALITY ASSURANCE

### Task 8.1: Unit Tests for Backend Services

**Description:** Write comprehensive unit tests for all streaming intelligence services.

**Acceptance Criteria:**
- [ ] Test TranscriptionBuffer: rolling window, trimming, formatting
- [ ] Test StreamRouter: message parsing, routing, error handling
- [ ] Test QuestionHandler: lifecycle, state transitions, aggregation from all four tiers
- [ ] Test ActionHandler: detection, accumulation, completeness scoring
- [ ] Test AnswerHandler: semantic matching, resolution
- [ ] Test GPTAnswerGenerator: answer generation, confidence scoring, fallback behavior
- [ ] Test SegmentDetector: boundary detection heuristics
- [ ] Achieve >80% code coverage
- [ ] Use pytest with async support

**Complexity:** Medium
**Dependencies:** Tasks 2.1-2.6, 3.4
**Priority:** P1

**Related Files:**
- Create: `/backend/tests/services/intelligence/test_*.py`

---

### Task 8.2: Integration Tests for Four-Tier Answer Discovery

**Description:** Write end-to-end integration tests for the complete answer discovery flow.

**Acceptance Criteria:**
- [ ] Test full flow: question detected â†’ RAG search â†’ meeting context search â†’ live monitoring â†’ GPT-generated answer
- [ ] Test progressive result delivery across all four tiers
- [ ] Test timeout handling (RAG 2s, meeting context 1.5s, live 15s, GPT generation 3s)
- [ ] Test graceful degradation when tiers fail
- [ ] Test tier priority (RAG > Meeting Context > Live > GPT-generated)
- [ ] Verify GPT-generated answers only trigger when other tiers fail
- [ ] Test concurrent questions
- [ ] Mock vector database and GPT API
- [ ] Write tests with realistic meeting scenarios

**Complexity:** Complex
**Dependencies:** Tasks 3.1, 3.2, 3.3, 3.4
**Priority:** P1

**Related Files:**
- Create: `/backend/tests/integration/test_answer_discovery.py`

---

### Task 8.3: WebSocket Communication Tests

**Description:** Write tests for WebSocket message protocol and real-time communication.

**Acceptance Criteria:**
- [ ] Test connection lifecycle: connect, authenticate, disconnect, reconnect
- [ ] Test all message types: QUESTION_DETECTED, RAG_RESULT, ANSWER_FROM_MEETING, QUESTION_ANSWERED_LIVE, GPT_GENERATED_ANSWER, ACTION_TRACKED, ACTION_UPDATED, ACTION_ALERT, etc.
- [ ] Test user feedback messages from client
- [ ] Test broadcast to multiple participants
- [ ] Test connection error handling
- [ ] Test rate limiting
- [ ] Use WebSocket test client

**Complexity:** Medium
**Dependencies:** Task 4.1
**Priority:** P1

**Related Files:**
- Create: `/backend/tests/routers/test_websocket_live_insights.py`

---

### Task 8.4: Flutter Widget Tests

**Description:** Write widget tests for all live insights UI components.

**Acceptance Criteria:**
- [ ] Test LiveQuestionCard: all states (searching, found via RAG/meeting/live/GPT), user interactions, animations, answer source display
- [ ] Test LiveActionCard: completeness display, user actions, inline editing
- [ ] Test AIAssistantContentSection: layout, scroll, empty states
- [ ] Test recording panel AI Assistant toggle
- [ ] Test real-time update handling without flicker
- [ ] Achieve >70% widget test coverage
- [ ] Use flutter_test and mockito for mocking

**Complexity:** Medium
**Dependencies:** Tasks 5.1, 5.2, 5.3, 5.4
**Priority:** P2

**Related Files:**
- Create: `/test/features/live_insights/presentation/widgets/test_*.dart`

---

## APPENDIX A: TASK DEPENDENCY GRAPH

```
Database Layer:
1.1 (Create Table) â†’ 1.2 (Create Model)

Backend Streaming Engine:
2.1 (Transcription Buffer) â†’ 2.3 (Stream Router)
2.2 (GPT Streaming) â†’ 2.3 (Stream Router)
2.3 (Stream Router) â†’ 2.4 (Question Handler)
2.3 (Stream Router) â†’ 2.5 (Action Handler)
2.3 (Stream Router) â†’ 2.6 (Answer Handler)

Four-Tier Answer Discovery:
2.4 (Question Handler) â†’ 3.1 (RAG Search)
2.4 (Question Handler) â†’ 3.2 (Meeting Context Search)
2.6 (Answer Handler) â†’ 3.3 (Live Monitoring)
3.3 (Live Monitoring) â†’ 3.4 (GPT Answer Generator)
2.1 (Transcription Buffer) â†’ 3.5 (Segment Detector)

WebSocket Integration:
2.4, 2.5, 2.6 â†’ 4.1 (WebSocket Router)
2.1 â†’ 4.2 (Redis Integration)

Flutter UI:
5.1 (AI Toggle) â†’ 5.2 (Questions Widget)
5.1 (AI Toggle) â†’ 5.3 (Actions Widget)
5.2, 5.3 â†’ 5.4 (Content Section)
4.1 â†’ 5.5 (State Management)
4.1 â†’ 5.7 (WebSocket Service)

GPT-5-mini:
6.1 (Provider) â†’ 6.2 (Prompts)

Orchestration:
2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 3.5 â†’ 7.1 (Orchestrator)
7.1 â†’ 7.2 (Recording Integration)

Testing:
2.1-2.6, 3.4, 3.5 â†’ 8.1 (Unit Tests)
3.1, 3.2, 3.3, 3.4 â†’ 8.2 (Integration Tests - Four Tiers)
4.1 â†’ 8.3 (WebSocket Tests)
5.1-5.4 â†’ 8.4 (Widget Tests)
```

---

## APPENDIX E: FOUR-TIER ANSWER DISCOVERY FLOW

### Tier Priority & Execution

```
Question Detected at t=0s
    â†“
    â”œâ”€â†’ Tier 1: RAG Search (parallel, 2s timeout)
    â”‚   â”œâ”€ Found documents â†’ Stream to UI with "ðŸ“š From Documents" label
    â”‚   â””â”€ No results â†’ Continue
    â”‚
    â”œâ”€â†’ Tier 2: Meeting Context Search (parallel, 1.5s timeout)
    â”‚   â”œâ”€ Found earlier answer â†’ Send to UI with "ðŸ’¬ Earlier in Meeting" label
    â”‚   â””â”€ No results â†’ Continue
    â”‚
    â”œâ”€â†’ Tier 3: Live Conversation Monitoring (15s window)
    â”‚   â”œâ”€ Answer detected in conversation â†’ Mark resolved with "ðŸ‘‚ Answered Live" label
    â”‚   â””â”€ No answer after 15s â†’ Continue to Tier 4
    â”‚
    â””â”€â†’ Tier 4: GPT-Generated Answer (3s timeout)
        â”œâ”€ GPT generates answer (confidence >70%) â†’ Display with "ðŸ¤– AI Answer" + disclaimer
        â””â”€ GPT cannot answer or low confidence â†’ Mark as UNANSWERED
```

### UI Answer Source Labels

**Purpose:** Users must clearly understand where each answer came from to assess reliability.

| Tier | Source Label | Icon | Color | Reliability Indicator |
|------|-------------|------|-------|----------------------|
| **Tier 1** | "From Documents" | ðŸ“š | Blue | High - From verified documents |
| **Tier 2** | "Earlier in Meeting" | ðŸ’¬ | Purple | High - From actual meeting discussion |
| **Tier 3** | "Answered Live" | ðŸ‘‚ | Green | High - Live conversation response |
| **Tier 4** | "AI Answer" | ðŸ¤– | Orange | Medium - AI-generated, needs verification |

### Tier 4 Disclaimer Requirements

When displaying GPT-generated answers:

1. **Prominent Badge:** "Generated by AI" badge next to answer
2. **Disclaimer Text:** "This answer was generated by AI based on general knowledge, not from your documents or meeting"
3. **Confidence Score:** Display GPT's self-assessed confidence (if >70%)
4. **Visual Distinction:** Use different background color or border to separate from other answer sources
5. **User Verification Prompt:** Encourage users to verify answer accuracy

### Answer Source Priority in Database

Store `answer_source` field in `live_meeting_insights` table:

```sql
answer_source VARCHAR CHECK (answer_source IN (
    'rag',              -- Tier 1: Found in documents
    'meeting_context',  -- Tier 2: Earlier in meeting
    'live_conversation',-- Tier 3: Live monitoring
    'gpt_generated',    -- Tier 4: AI-generated
    'user_provided',    -- User manually marked as answered
    'unanswered'        -- No answer found
))
```

### Example UI Mockup (Question Card)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Q: "What's our Q4 budget for infrastructure?"      â”‚
â”‚ Asked by Sarah at 10:30:05                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚ ðŸ“š From Documents (2 results)                      â”‚
â”‚ â”œâ”€ "Infrastructure Budget Q4 2025.pdf"             â”‚
â”‚ â””â”€ "Annual Planning Document.docx"                 â”‚
â”‚                                                     â”‚
â”‚ ðŸ’¬ Earlier in Meeting                               â”‚
â”‚ â””â”€ No prior discussion                             â”‚
â”‚                                                     â”‚
â”‚ ðŸ¤– AI Answer (Confidence: 75%)                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ âš ï¸ Generated by AI - Please verify            â”‚ â”‚
â”‚ â”‚                                               â”‚ â”‚
â”‚ â”‚ Based on general knowledge, typical Q4        â”‚ â”‚
â”‚ â”‚ infrastructure budgets range from...          â”‚ â”‚
â”‚ â”‚                                               â”‚ â”‚
â”‚ â”‚ â“˜ This answer was not found in your          â”‚ â”‚
â”‚ â”‚   documents or meeting discussion             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚ [Mark as Answered] [Needs Follow-up] [Dismiss]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**End of Tasks Document**
